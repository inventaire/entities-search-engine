#!/usr/bin/env coffee

# Pass the path of a results json file, supposedly made of an array of ids:
# all those ids documents will be deleted
[ resultPath ] = process.argv.slice(2)
{ host:elasticHost, indexes } = require('config').elastic
{ wikidata:index } = indexes

resolvePath = require './lib/resolve_path'
ids = require resolvePath(resultPath)
breq = require 'bluereq'
_ = require '../lib/utils'

type = resultPath.split('/').slice(-1)[0].split('.')[0]
_.info type, 'type'
total = ids.length
_.info total, 'ids'

# TODO: use the Bulk API
# https://www.elastic.co/guide/en/elasticsearch/reference/2.3/docs-bulk.html
deleteNextBulk = ->
  idsBatch = ids.splice(0, 1000)

  body = idsBatch.map(metaDataLine).join '\n'

  breq.post "http://localhost:9200/_bulk", body
  .then ->
    _.success idsBatch, 'deleted'
    _.info ids.length, 'remaining'
    if ids.length > 0 then setTimeout deleteNextBulk, 200
  .catch (err)->
    # if 404, continue as the document might just
    # have been manully deleted earlier
    if err.statusCode is 404
      _.warn idsBatch, '404'
      setTimeout deleteNextBulk, 200
    else
      _.error err, 'err'

metaDataLine = (id)->
  "{\"delete\":{\"_index\":\"#{index}\",\"_type\":\"#{type}\",\"_id\":\"#{id}\"}}"

# let 5 seconds to kill the process if something is wrong
_.info 'starting in 5 seconds...'
setTimeout deleteNextBulk, 5000
